{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importiamo le librerie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import AutoTokenizer, AutoModel, BertTokenizer, BertModel, BertConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sn\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import math\n",
    "from scipy.stats import wilcoxon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funzione che spacchetta gli alberi**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def unplickle_trees(path_tree_file):\n",
    "    print('--->read DTKs')\n",
    "    dt_trees = []\n",
    "    with open(path_tree_file, 'rb') as fr:\n",
    "        try:\n",
    "            while True:\n",
    "                dt_trees.append(pickle.load(fr))\n",
    "        except EOFError:\n",
    "            pass\n",
    "    return [torch.FloatTensor(i) for i in dt_trees]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importiamo il dataset e gli alberi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--->read DTKs\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>autore</th>\n",
       "      <th>categoria</th>\n",
       "      <th>data</th>\n",
       "      <th>patologia</th>\n",
       "      <th>rating</th>\n",
       "      <th>reparto</th>\n",
       "      <th>struttura</th>\n",
       "      <th>testo</th>\n",
       "      <th>titolo</th>\n",
       "      <th>filename</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Silvia Grotto</td>\n",
       "      <td>Abrasione cerebrale</td>\n",
       "      <td>09 Agosto, 2013</td>\n",
       "      <td>Aneurisma cerebrale tipo Blister.</td>\n",
       "      <td>{'Voto medio': 4.3, 'Competenza': 5.0, 'Assist...</td>\n",
       "      <td>Neuroradiologia Ospedale Vicenza</td>\n",
       "      <td>Ospedale San Bortolo di Vicenza</td>\n",
       "      <td>Siamo tutti abituati a pensare ai Super Eroi d...</td>\n",
       "      <td>Neuroradiologia S.Bortolo: eccellenza</td>\n",
       "      <td>Malattie del cervello e del sistema nervoso</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gilda fabiani</td>\n",
       "      <td>Abrasione cerebrale</td>\n",
       "      <td>24 Dicembre, 2011</td>\n",
       "      <td>Abrasione cerebrale.</td>\n",
       "      <td>{'Voto medio': 4.8, 'Competenza': 5.0, 'Assist...</td>\n",
       "      <td>Neurochirurgia Ospedale Terni</td>\n",
       "      <td>Ospedale Santa Maria di Terni</td>\n",
       "      <td>Un ringraziamento speciale al dott. Carletti e...</td>\n",
       "      <td>Operazione al cervello- dott. Carletti</td>\n",
       "      <td>Malattie del cervello e del sistema nervoso</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rescigno Andrea</td>\n",
       "      <td>Adenoma ipofisario</td>\n",
       "      <td>15 Agosto, 2021</td>\n",
       "      <td>Macroadenoma ipofisario.</td>\n",
       "      <td>{'Voto medio': 5.0, 'Competenza': 5.0, 'Assist...</td>\n",
       "      <td>Neurochirurgia Ospedale Caserta</td>\n",
       "      <td>Ospedale di Caserta</td>\n",
       "      <td>Desidero esprimere il mio ringraziamento a tut...</td>\n",
       "      <td>Ringraziamenti al Reparto di Neurochirurgia</td>\n",
       "      <td>Malattie del cervello e del sistema nervoso</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stan Marian</td>\n",
       "      <td>Adenoma ipofisario</td>\n",
       "      <td>29 Luglio, 2021</td>\n",
       "      <td>Adenoma ipofisario.</td>\n",
       "      <td>{'Voto medio': 5.0, 'Competenza': 5.0, 'Assist...</td>\n",
       "      <td>Neurochirurgia Ospedale Treviso</td>\n",
       "      <td>Ospedale S. Maria di Ca' Foncello di Treviso</td>\n",
       "      <td>Volevo ringraziare per la gentilezza e per i c...</td>\n",
       "      <td>Grazie Dott. Jacopo Del Verme</td>\n",
       "      <td>Malattie del cervello e del sistema nervoso</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Lucia</td>\n",
       "      <td>Adenoma ipofisario</td>\n",
       "      <td>23 Luglio, 2021</td>\n",
       "      <td>Macroadenoma ipofisario.</td>\n",
       "      <td>{'Voto medio': 4.0, 'Competenza': 1.0, 'Assist...</td>\n",
       "      <td>Neurochirurgia Policlinico Gemelli</td>\n",
       "      <td>Ospedale Policlinico Agostino Gemelli di Roma</td>\n",
       "      <td>Se potessi dare 10 come voto al professore Vis...</td>\n",
       "      <td>Grazie professore M. Visocchi</td>\n",
       "      <td>Malattie del cervello e del sistema nervoso</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            autore            categoria               data  \\\n",
       "0    Silvia Grotto  Abrasione cerebrale    09 Agosto, 2013   \n",
       "1    gilda fabiani  Abrasione cerebrale  24 Dicembre, 2011   \n",
       "2  Rescigno Andrea   Adenoma ipofisario    15 Agosto, 2021   \n",
       "3     Stan Marian    Adenoma ipofisario    29 Luglio, 2021   \n",
       "4            Lucia   Adenoma ipofisario    23 Luglio, 2021   \n",
       "\n",
       "                             patologia  \\\n",
       "0  Aneurisma cerebrale tipo Blister.     \n",
       "1                 Abrasione cerebrale.   \n",
       "2            Macroadenoma ipofisario.    \n",
       "3                  Adenoma ipofisario.   \n",
       "4             Macroadenoma ipofisario.   \n",
       "\n",
       "                                              rating  \\\n",
       "0  {'Voto medio': 4.3, 'Competenza': 5.0, 'Assist...   \n",
       "1  {'Voto medio': 4.8, 'Competenza': 5.0, 'Assist...   \n",
       "2  {'Voto medio': 5.0, 'Competenza': 5.0, 'Assist...   \n",
       "3  {'Voto medio': 5.0, 'Competenza': 5.0, 'Assist...   \n",
       "4  {'Voto medio': 4.0, 'Competenza': 1.0, 'Assist...   \n",
       "\n",
       "                              reparto  \\\n",
       "0    Neuroradiologia Ospedale Vicenza   \n",
       "1       Neurochirurgia Ospedale Terni   \n",
       "2     Neurochirurgia Ospedale Caserta   \n",
       "3     Neurochirurgia Ospedale Treviso   \n",
       "4  Neurochirurgia Policlinico Gemelli   \n",
       "\n",
       "                                       struttura  \\\n",
       "0                Ospedale San Bortolo di Vicenza   \n",
       "1                  Ospedale Santa Maria di Terni   \n",
       "2                            Ospedale di Caserta   \n",
       "3   Ospedale S. Maria di Ca' Foncello di Treviso   \n",
       "4  Ospedale Policlinico Agostino Gemelli di Roma   \n",
       "\n",
       "                                               testo  \\\n",
       "0  Siamo tutti abituati a pensare ai Super Eroi d...   \n",
       "1  Un ringraziamento speciale al dott. Carletti e...   \n",
       "2  Desidero esprimere il mio ringraziamento a tut...   \n",
       "3  Volevo ringraziare per la gentilezza e per i c...   \n",
       "4  Se potessi dare 10 come voto al professore Vis...   \n",
       "\n",
       "                                        titolo  \\\n",
       "0        Neuroradiologia S.Bortolo: eccellenza   \n",
       "1       Operazione al cervello- dott. Carletti   \n",
       "2  Ringraziamenti al Reparto di Neurochirurgia   \n",
       "3                Grazie Dott. Jacopo Del Verme   \n",
       "4                Grazie professore M. Visocchi   \n",
       "\n",
       "                                      filename  target  \n",
       "0  Malattie del cervello e del sistema nervoso       1  \n",
       "1  Malattie del cervello e del sistema nervoso       1  \n",
       "2  Malattie del cervello e del sistema nervoso       1  \n",
       "3  Malattie del cervello e del sistema nervoso       1  \n",
       "4  Malattie del cervello e del sistema nervoso       1  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.read_csv('HR_dataset_NEW.csv')\n",
    "trees1 = unplickle_trees('alberi_fine_FF.pkl')\n",
    "\n",
    "df = df1[:30000]\n",
    "trees = trees1[:30000]\n",
    "\n",
    "del df1\n",
    "del trees1\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30000, 30000)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(df),len(trees)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1  1] [ 1843 28157]\n"
     ]
    }
   ],
   "source": [
    "sentences = df.testo.values\n",
    "labels = df.target.values\n",
    "\n",
    "unique, counts = np.unique(labels, return_counts = True)\n",
    "\n",
    "print(unique, counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Funzioni utili per il calcolo statistico**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mean(number_list):\n",
    "    sum = 0\n",
    "    for number in number_list:\n",
    "        sum += number\n",
    "\n",
    "    return sum/len(number_list)\n",
    "\n",
    "def calculate_standard_deviation(number_list):\n",
    "    mean = calculate_mean(number_list)\n",
    "    summatory = 0\n",
    "    for number in number_list:\n",
    "        summatory += pow((number - mean),2)\n",
    "\n",
    "    summatory = summatory/len(number_list)\n",
    "\n",
    "    return math.sqrt(summatory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Italians Bert**\n",
    "* **Italian Bert**\n",
    "    * Tokenizzatore -->  BertTokenizer.from_pretrained(\"dbmdz/bert-base-italian-uncased\", do_lower_case=True)\n",
    "    * Modello --> BertModel.from_pretrained('dbmdz/bert-base-italian-uncased')\n",
    "\n",
    "\n",
    "\n",
    "* **Umberto** \n",
    "    * Tokenizzatore --> AutoTokenizer.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\")\n",
    "    * Modello --> AutoModel.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\")\n",
    "\n",
    "\n",
    "\n",
    "* **Alberto** \n",
    "    * Tokenizzatore --> AutoTokenizer.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n",
    "    * Modello --> AutoModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n",
    "    \n",
    "    \n",
    "\n",
    "* **Multilingua-Bert** \n",
    "    * Tokenizzatore --> BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "    * Modello --> BertModel.from_pretrained(\"bert-base-multilingual-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_input(seed, random_state, sentences, model_type, epochs):\n",
    "    \n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    if torch.cuda.is_available(): torch.cuda.manual_seed_all(seed)\n",
    "        \n",
    "    global tokenizer\n",
    "    global model_architecture\n",
    "        \n",
    "    sentences = [\"[CLS] \" + sentence + \" [SEP]\" for sentence in sentences]\n",
    "    \n",
    "    if model_type == 'Italian_Bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"dbmdz/bert-base-italian-uncased\", do_lower_case=True)\n",
    "        model_architecture = BertModel.from_pretrained('dbmdz/bert-base-italian-uncased').to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if model_type == 'Umberto':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\")\n",
    "        model_architecture = AutoModel.from_pretrained(\"Musixmatch/umberto-commoncrawl-cased-v1\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "                \n",
    "    if model_type == 'Alberto':\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\")\n",
    "        model_architecture = AutoModel.from_pretrained(\"m-polignano-uniba/bert_uncased_L-12_H-768_A-12_italian_alb3rt0\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    if model_type == 'Multilingua-Bert':\n",
    "        tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-uncased')\n",
    "        model_architecture = BertModel.from_pretrained(\"bert-base-multilingual-uncased\").to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
    "    \n",
    "    MAX_LEN = 64\n",
    "    input_ids = pad_sequences([tokenizer.convert_tokens_to_ids(txt) for txt in tokenized_texts],\n",
    "                          maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "    \n",
    "    attention_masks = []\n",
    "\n",
    "    for seq in input_ids:\n",
    "        seq_mask = [float(i>0) for i in seq]\n",
    "        attention_masks.append(seq_mask)\n",
    "        \n",
    "    X_inputs, test_inputs, X_labels, test_labels = train_test_split(input_ids, labels, random_state=random_state, test_size=0.2)\n",
    "    X_masks, test_masks, _, _ = train_test_split(attention_masks, input_ids, random_state=random_state, test_size=0.2)\n",
    "    X_trees, test_trees, _, _ = train_test_split(trees, input_ids, random_state=random_state, test_size=0.2)\n",
    "\n",
    "    train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(X_inputs, X_labels, random_state=random_state, test_size=0.3)\n",
    "    train_masks, validation_masks, _, _ = train_test_split(X_masks, X_inputs, random_state=random_state, test_size=0.3)\n",
    "    train_trees, validation_trees, _, _ = train_test_split(X_trees, X_inputs, random_state=random_state, test_size=0.3)\n",
    "\n",
    "    train_inputs = torch.tensor(train_inputs)\n",
    "    train_labels = torch.tensor(train_labels)\n",
    "    train_masks = torch.tensor(train_masks)\n",
    "    train_trees = torch.stack(train_trees)\n",
    "\n",
    "    validation_inputs = torch.tensor(validation_inputs)\n",
    "    validation_labels = torch.tensor(validation_labels)\n",
    "    validation_masks = torch.tensor(validation_masks)\n",
    "    validation_trees = torch.stack(validation_trees)\n",
    "\n",
    "    test_inputs = torch.tensor(test_inputs)\n",
    "    test_labels = torch.tensor(test_labels)\n",
    "    test_masks = torch.tensor(test_masks)\n",
    "    test_trees = torch.stack(test_trees)\n",
    "\n",
    "    batch_size = 32\n",
    "\n",
    "    train_data = TensorDataset(train_inputs, train_masks, train_trees, train_labels)\n",
    "    train_sampler = RandomSampler(train_data)\n",
    "    train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "\n",
    "    validation_data = TensorDataset(validation_inputs, validation_masks, validation_trees, validation_labels)\n",
    "    validation_sampler = SequentialSampler(validation_data)\n",
    "    validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n",
    "\n",
    "    test_data = TensorDataset(test_inputs, test_masks, test_trees, test_labels)\n",
    "    test_sampler = SequentialSampler(test_data)\n",
    "    test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)\n",
    "    \n",
    "    return train_dataloader, validation_dataloader, test_dataloader, model_architecture, device, test_labels\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definiamo i modelli che utilizzeremo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solo Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertForSequenceClassification(nn.Module):\n",
    "  \n",
    "    def __init__(self, input_dim_bert, output_dim, model_architecture):\n",
    "        super().__init__()\n",
    "        self.bert = model_architecture\n",
    "        \n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.sem_linear = nn.Linear(input_dim_bert, output_dim)\n",
    "        \n",
    "    def forward(self, x_sem, attention_mask):\n",
    "        with torch.no_grad():\n",
    "            pooled_output = self.bert(x_sem, attention_mask)[0][:, 0, :]  \n",
    "            pooled_output = self.dropout(pooled_output)\n",
    "        logits = self.sem_linear(pooled_output)\n",
    "\n",
    "        return logits\n",
    "        \n",
    "    def get_activation(self, x_sem, x_synth):\n",
    "        with torch.no_grad():\n",
    "            x_sem = self.bert(x_sem)[0][:, 0, :]\n",
    "            x_tot = torch.cat((x_sem, x_synth), 1)\n",
    "            x_tot = self.synth_sem_linear(x_tot)\n",
    "        out = F.log_softmax(x_tot, dim=1)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creiamo le funzioni di applicazione dei modelli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Esecuzione di solo Bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def execute_Bert(epochs, model_architecture, train_dataloader, validation_dataloader, test_dataloader, device, test_labels):\n",
    "    Alone_model = BertForSequenceClassification(768,2, model_architecture)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    parameters = filter(lambda p: p.requires_grad, Alone_model.parameters())\n",
    "    optimizer = optim.AdamW(Alone_model.parameters(), lr=2e-5)\n",
    "\n",
    "    Alone_model.cuda()\n",
    "\n",
    "    # Store our loss and accuracy for plotting\n",
    "    train_loss_set = []\n",
    "    # Number of training epochs \n",
    "    epoch = 0\n",
    "\n",
    "    # BERT training loop\n",
    "    for _ in trange(epochs, desc=\"Epoch\"):  \n",
    "        Alone_model.train()  \n",
    "          # Tracking variables\n",
    "        tr_loss = 0\n",
    "        nb_tr_examples, nb_tr_steps = 0, 0\n",
    "      # Train the data for one epoch\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            #print(step, batch)\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.cuda() for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask,b_input_tree, b_labels = batch\n",
    "            # Clear out the gradients (by default they accumulate)\n",
    "            optimizer.zero_grad()\n",
    "            # Forward pass\n",
    "\n",
    "            #NB anche BertForSequenceClassification prende in input anche b_input_tree ma non li usa (solo per comodità)\n",
    "            target_hat = Alone_model(b_input_ids, b_input_mask)\n",
    "\n",
    "            loss = criterion(target_hat, b_labels)\n",
    "            train_loss_set.append(loss.item())\n",
    "\n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            # Update parameters and take a step using the computed gradient\n",
    "            optimizer.step()\n",
    "            # Update tracking variables\n",
    "            tr_loss += loss.item()\n",
    "            nb_tr_examples += b_input_ids.size(0)\n",
    "            nb_tr_steps += 1\n",
    "\n",
    "        ## VALIDATION\n",
    "\n",
    "      # Put model in evaluation mode\n",
    "        Alone_model.eval()\n",
    "        # Tracking variables \n",
    "        eval_loss, eval_accuracy = 0, 0\n",
    "        nb_eval_steps, nb_eval_examples = 0, 0\n",
    "        # Evaluate data for one epoch\n",
    "        for batch in validation_dataloader:\n",
    "            # Add batch to GPU\n",
    "            batch = tuple(t.cuda() for t in batch)\n",
    "            # Unpack the inputs from our dataloader\n",
    "            b_input_ids, b_input_mask, b_input_tree, b_labels = batch\n",
    "            # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "            with torch.no_grad():\n",
    "\n",
    "              # Forward pass, calculate logit predictions\n",
    "\n",
    "            #NB anche BertForSequenceClassification prende in input anche b_input_tree ma non li usa (solo per comodità)\n",
    "              logits = Alone_model(b_input_ids, b_input_mask)\n",
    "\n",
    "            # Move logits and labels to CPU\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = b_labels.to('cpu').numpy()\n",
    "            tmp_eval_accuracy = flat_accuracy(logits, label_ids)    \n",
    "            eval_accuracy += tmp_eval_accuracy\n",
    "            nb_eval_steps += 1\n",
    "\n",
    "        epoch +=1\n",
    "\n",
    "    predictions = []\n",
    "    Alone_model.eval()\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_input_tree, b_labels = batch\n",
    "\n",
    "        with torch.no_grad():\n",
    "            logits = Alone_model(b_input_ids, b_input_mask)\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "\n",
    "        predictions.append(logits)\n",
    "\n",
    "        flat_predictions = [item for sublist in predictions for item in sublist]\n",
    "        flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
    "        \n",
    "    A = accuracy_score(test_labels.numpy(), flat_predictions)\n",
    "    B = f1_score(test_labels.numpy(), flat_predictions, average='macro')\n",
    "    C = f1_score(test_labels.numpy(), flat_predictions, average='weighted')\n",
    "    D = f1_score(test_labels.numpy(), flat_predictions, average=None)\n",
    "        \n",
    "    return A,B,C,D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eseguiamo le funzioni"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Definiamo le varibili che univoche per tutti i modelli**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = [46, 23, 17, 54, 31, 54, 1, 28, 52, 27]\n",
    "random_state = [1024, 3333, 1995, 2780, 3833, 1394, 779, 4787, 845, 5480]\n",
    "model_architecture_list = ['Italian_Bert','Umberto','Alberto', 'Multilingua-Bert']\n",
    "epochs = 2\n",
    "\n",
    "model_architecture = model_architecture_list[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Eseguiamo solo Bert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/2 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA error: device-side assert triggered",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-9d7002ae377c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mtrain_dataloder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_architecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefine_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_architecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mA\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mB\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecute_Bert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_architecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mBert_accuracy_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-e3d578755a88>\u001b[0m in \u001b[0;36mexecute_Bert\u001b[0;34m(epochs, model_architecture, train_dataloader, validation_dataloader, test_dataloader, device, test_labels)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_hat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m             \u001b[0mtrain_loss_set\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0;31m# Backward pass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered"
     ]
    }
   ],
   "source": [
    "Bert_accuracy_list = []\n",
    "Bert_macro_list = []\n",
    "Bert_weighted_list = []\n",
    "Bert_other_0 = []\n",
    "Bert_other_1 = []\n",
    "\n",
    "\n",
    "for i in range(0, 10):\n",
    "    train_dataloder, validation_dataloader, test_dataloder, model_architecture, device, test_labels = define_input(seed[i], random_state[i], sentences, model_architecture, epochs)\n",
    "    \n",
    "    A,B,C,D = execute_Bert(epochs, model_architecture, train_dataloder, validation_dataloader, test_dataloder, device, test_labels)\n",
    "    \n",
    "    Bert_accuracy_list.append(A)\n",
    "    Bert_macro_list.append(B)\n",
    "    Bert_weighted_list.append(C)\n",
    "    Bert_other_0.append(D[0])\n",
    "    Bert_other_1.append(D[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calcoliamo la media e la variazione standard dei 3 modelli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kermit Potter**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** F1 - SCORE CLASS 0\n",
      "0.22\n",
      "0.09\n",
      "*********** F1 - SCORE CLASS 1\n",
      "0.89\n",
      "0.08\n",
      "*********** ACCURACY\n",
      "0.81\n",
      "0.12\n",
      "*********** MACRO\n",
      "0.55\n",
      "0.07\n",
      "*********** WEIGHTED\n",
      "0.84\n",
      "0.08\n"
     ]
    }
   ],
   "source": [
    "print('*********** F1 - SCORE CLASS 0')\n",
    "print(round(calculate_mean(Potter_other_0), 2))\n",
    "print(round(calculate_standard_deviation(Potter_other_0), 2))\n",
    "\n",
    "print('*********** F1 - SCORE CLASS 1')\n",
    "print(round(calculate_mean(Potter_other_1), 2))\n",
    "print(round(calculate_standard_deviation(Potter_other_1), 2))\n",
    "\n",
    "print('*********** ACCURACY')\n",
    "print(round(calculate_mean(Potter_accuracy_list),2))\n",
    "print(round(calculate_standard_deviation(Potter_accuracy_list),2))\n",
    "\n",
    "print('*********** MACRO')\n",
    "print(round(calculate_mean(Potter_macro_list),2))\n",
    "print(round(calculate_standard_deviation(Potter_macro_list),2))\n",
    "\n",
    "print('*********** WEIGHTED')\n",
    "print(round(calculate_mean(Potter_weighted_list), 2))\n",
    "print(round(calculate_standard_deviation(Potter_weighted_list), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kermit classico**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** F1 - SCORE CLASS 0\n",
      "0.23\n",
      "0.07\n",
      "*********** F1 - SCORE CLASS 1\n",
      "0.9\n",
      "0.05\n",
      "*********** ACCURACY\n",
      "0.82\n",
      "0.08\n",
      "*********** MACRO\n",
      "0.56\n",
      "0.05\n",
      "*********** WEIGHTED\n",
      "0.85\n",
      "0.05\n"
     ]
    }
   ],
   "source": [
    "print('*********** F1 - SCORE CLASS 0')\n",
    "print(round(calculate_mean(Kermit_other_0), 2))\n",
    "print(round(calculate_standard_deviation(Kermit_other_0), 2))\n",
    "\n",
    "print('*********** F1 - SCORE CLASS 1')\n",
    "print(round(calculate_mean(Kermit_other_1), 2))\n",
    "print(round(calculate_standard_deviation(Kermit_other_1), 2))\n",
    "\n",
    "print('*********** ACCURACY')\n",
    "print(round(calculate_mean(Kermit_accuracy_list),2))\n",
    "print(round(calculate_standard_deviation(Kermit_accuracy_list),2))\n",
    "\n",
    "print('*********** MACRO')\n",
    "print(round(calculate_mean(Kermit_macro_list),2))\n",
    "print(round(calculate_standard_deviation(Kermit_macro_list),2))\n",
    "\n",
    "print('*********** WEIGHTED')\n",
    "print(round(calculate_mean(Kermit_weighted_list), 2))\n",
    "print(round(calculate_standard_deviation(Kermit_weighted_list), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Solo Bert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*********** F1 - SCORE CLASS 0\n",
      "0.05\n",
      "0.08\n",
      "*********** F1 - SCORE CLASS 1\n",
      "0.91\n",
      "0.1\n",
      "*********** ACCURACY\n",
      "0.85\n",
      "0.14\n",
      "*********** MACRO\n",
      "0.48\n",
      "0.04\n",
      "*********** WEIGHTED\n",
      "0.84\n",
      "0.09\n"
     ]
    }
   ],
   "source": [
    "print('*********** F1 - SCORE CLASS 0')\n",
    "print(round(calculate_mean(Bert_other_0), 2))\n",
    "print(round(calculate_standard_deviation(Bert_other_0), 2))\n",
    "\n",
    "print('*********** F1 - SCORE CLASS 1')\n",
    "print(round(calculate_mean(Bert_other_1), 2))\n",
    "print(round(calculate_standard_deviation(Bert_other_1), 2))\n",
    "\n",
    "print('*********** ACCURACY')\n",
    "print(round(calculate_mean(Bert_accuracy_list),2))\n",
    "print(round(calculate_standard_deviation(Bert_accuracy_list),2))\n",
    "\n",
    "print('*********** MACRO')\n",
    "print(round(calculate_mean(Bert_macro_list),2))\n",
    "print(round(calculate_standard_deviation(Bert_macro_list),2))\n",
    "\n",
    "print('*********** WEIGHTED')\n",
    "print(round(calculate_mean(Bert_weighted_list), 2))\n",
    "print(round(calculate_standard_deviation(Bert_weighted_list), 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verifichiamo se i risultati ottenuti sono statisticamente significativi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_test(p):\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print('Same distribution (fail to reject H0)')\n",
    "    else:\n",
    "        print('Different distribution (reject H0)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Potter vs Kermit**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_w, a_p = wilcoxon(Potter_accuracy_list, Kermit_accuracy_list)\n",
    "m_w, m_p = wilcoxon(Potter_macro_list, Kermit_macro_list)\n",
    "w_w, w_p = wilcoxon(Potter_weighted_list, Kermit_weighted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same distribution (fail to reject H0)\n",
      "Same distribution (fail to reject H0)\n",
      "Same distribution (fail to reject H0)\n"
     ]
    }
   ],
   "source": [
    "p_test(a_p)\n",
    "p_test(m_p)\n",
    "p_test(w_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Potter vs Bert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_w, a_p = wilcoxon(Potter_accuracy_list, Bert_accuracy_list)\n",
    "m_w, m_p = wilcoxon(Potter_macro_list, Bert_macro_list)\n",
    "w_w, w_p = wilcoxon(Potter_weighted_list, Bert_weighted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same distribution (fail to reject H0)\n",
      "Different distribution (reject H0)\n",
      "Same distribution (fail to reject H0)\n"
     ]
    }
   ],
   "source": [
    "p_test(a_p)\n",
    "p_test(m_p)\n",
    "p_test(w_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Kermit vs Bert**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_w, a_p = wilcoxon(Kermit_accuracy_list, Bert_accuracy_list)\n",
    "m_w, m_p = wilcoxon(Kermit_macro_list, Bert_macro_list)\n",
    "w_w, w_p = wilcoxon(Kermit_weighted_list, Bert_weighted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Same distribution (fail to reject H0)\n",
      "Different distribution (reject H0)\n",
      "Same distribution (fail to reject H0)\n"
     ]
    }
   ],
   "source": [
    "p_test(a_p)\n",
    "p_test(m_p)\n",
    "p_test(w_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
